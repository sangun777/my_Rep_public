{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0. 프로젝트 시작 전, 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>파주시청. 사진제공=파주시 파주시청. 사진제공=파주시\\n\\n[파주=파이낸셜뉴스 강근...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>동영상 뉴스\\n\\n이천 물류창고 화재 발화지점으로 지목된 지하 2층에서 산소절단기의...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>황범순 의정부시 부시장 을지대학교 의정부캠퍼스 및 부속병원 공사현장 안전점검. 사진...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>귀갓길 여성을 쫓아가 성범죄를 시도한 20대 남성이 구속됐습니다.서울 강남경찰서는 ...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(서울=연합뉴스) 대한약사회가 6일부터 코로나바이러스 감염증 대응 체계를 '사회적 ...</td>\n",
       "      <td>사회</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news code\n",
       "0  파주시청. 사진제공=파주시 파주시청. 사진제공=파주시\\n\\n[파주=파이낸셜뉴스 강근...   사회\n",
       "1  동영상 뉴스\\n\\n이천 물류창고 화재 발화지점으로 지목된 지하 2층에서 산소절단기의...   사회\n",
       "2  황범순 의정부시 부시장 을지대학교 의정부캠퍼스 및 부속병원 공사현장 안전점검. 사진...   사회\n",
       "3  귀갓길 여성을 쫓아가 성범죄를 시도한 20대 남성이 구속됐습니다.서울 강남경찰서는 ...   사회\n",
       "4  (서울=연합뉴스) 대한약사회가 6일부터 코로나바이러스 감염증 대응 체계를 '사회적 ...   사회"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라이브러리 및 패키지 호출(os, pandas)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 저장해 둔 .csv 파일을 읽어서 DF 파일로 저장\n",
    "\n",
    "# 변수에 파일 경로 저장\n",
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "news_data_df = pd.read_table(csv_path, sep=',')\n",
    "news_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    파주시청 사진제공파주시 파주시청 사진제공파주시파주파이낸셜뉴스 강근주 기자 파주시는 ...\n",
       "1    동영상 뉴스이천 물류창고 화재 발화지점으로 지목된 지하 층에서 산소절단기의 산소 공...\n",
       "2    황범순 의정부시 부시장 을지대학교 의정부캠퍼스 및 부속병원 공사현장 안전점검 사진제...\n",
       "3    귀갓길 여성을 쫓아가 성범죄를 시도한 대 남성이 구속됐습니다서울 강남경찰서는 강간상...\n",
       "4    서울연합뉴스 대한약사회가 일부터 코로나바이러스 감염증 대응 체계를 사회적 거리두기에...\n",
       "Name: news, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거\n",
    "news_data_df['news'] = news_data_df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "news_data_df['news'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news    0\n",
      "code    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Null 값이 있는지 확인\n",
    "print(news_data_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 기사의 개수:  5124\n",
      "뉴스 기사의 개수:  3994\n"
     ]
    }
   ],
   "source": [
    "# 중복 샘플 제거\n",
    "\n",
    "# 중복 제거 전 기사 갯수\n",
    "print('뉴스 기사의 개수: ',len(news_data_df))\n",
    "\n",
    "# 중복 제거\n",
    "news_data_df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "# 중복 제거 후 기사 갯수\n",
    "print('뉴스 기사의 개수: ',len(news_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f62b9fccf10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Matplotlib 확인해서 카테고리 별 샘플 분포 확인\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "news_data_df['code'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    code  count\n",
      "0  IT/과학    903\n",
      "1     사회   1668\n",
      "2  생활/문화   1423\n"
     ]
    }
   ],
   "source": [
    "# 각 카테고리 별 뉴스기사 수 확인\n",
    "print(news_data_df.groupby('code').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 형태소 분석기 변경해 보기 1. Hannanum \n",
    "\n",
    "* 한국어 자연어 처리에서는 어떤 형태소 분류기를 사용하는지가 성능에 영향을 줌\n",
    "* konlpy 패키지에는 Mecab 이외에 Hannanum, Kkma, Komoran, Okt 라는 형태소분석기가 존재\n",
    "* 각 형태소 분석기 별 어떤것을 선택하는 것이 좋은지?\n",
    "* 분석기 간 장단점은 무엇인지?\n",
    "* 처리 속도와 성능에는 어떤 변화가 있는지? 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "\n",
    "# 1. Hannanum 형태소 분석기 사용\n",
    "\n",
    "# 분석기 불러오기\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "# 토큰 분석기 변수에 저장\n",
    "tokenizer = Hannanum()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['에','는','은','을','했','에게','있','#','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "        \n",
    "    temp_data = []\n",
    "    \n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer.morphs(sentence)\n",
    "    \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주시청 사진제공파주시 파주시청 사진제공파주시파주파이낸셜뉴스 강근주 기 이 자 파 아 주 시는 일 관내 취약계층 만가구 대 어 하 어 정부 긴급재난지원금 입금 완료했다파주시민 이 받 긴급재난지원금 일 ㄴ 이상 가구 기준 으로 만원 받 게 되 며 일 ㄴ 가구 만원 일 ㄴ 가구 만원 일 ㄴ 가구 만원이다정부 발표 하 ㄴ 긴급재난지원금 이 파주시민 지급 하 ㄴ 금액 과 다르 ㄴ 이유 국비지방비 부담 비율 때문이다파주시 이미 모든 시민 경기도파주시 재난기본소득인당 각 만원 지급 하고 어 시민 국비 지원금 만 지급 하 며 일 ㄴ 가구 기준 으로 총 지원 금액 파주시 재난기본소득 만원 경기 도 재난기본소득 만원 정부 긴급재난지원금 만원 총 만원 받 게 된다취약계층 이 아니 ㄴ 시민 오 월일 부터 소 이 지 하고 신용체크카드사 홈페이지 에서 긴급재난지원금 지원 신청 하 ㄹ 수 다 세대주 가족 지원금 일괄 신청 하 어야 한다한편 파 아 주 시는 일 김정기 부시장 단장 으로 하 긴급재난지원금 추진 태스크포스 를 구성해 긴급재난지원금 이 원활 하 게 지급 되 ㄹ 수 도록 지원 하 ㄴ다 저작권자 파이낸셜뉴스 전재재배포\n"
     ]
    }
   ],
   "source": [
    "text_data_1 = preprocessing(news_data_df['news'])\n",
    "print(text_data_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스 기사의 개수 : 2995\n",
      "테스트용 뉴스 기사의 개수 :  999\n",
      "훈련용 레이블의 개수 :  2995\n",
      "테스트용 레이블의 개수 :  999\n"
     ]
    }
   ],
   "source": [
    "# 사이킷 런 라이브러리를 이용하여 데이터 셋 분리\n",
    "\n",
    "# 라이브러리 및 패키지 호출\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data_1, news_data_df['code'], random_state = 0)\n",
    "\n",
    "# 데이터 출력\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사회']\n"
     ]
    }
   ],
   "source": [
    "# 나이브 베이즈 분류기 사용 예시 1 - 사회\n",
    "new_sent = preprocessing([\"민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \\\n",
    "                           주장이 나오는 데 대해 “체계·자구 심사가 법안 지연의 수단으로 \\\n",
    "                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\\\n",
    "                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['생활/문화']\n"
     ]
    }
   ],
   "source": [
    "# 나이브 베이즈 분류기 사용 예시 2 - 생활/문화\n",
    "new_sent = preprocessing([\"인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \\\n",
    "                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \\\n",
    "                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \\\n",
    "                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\\\n",
    "                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\\\n",
    "                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \\\n",
    "                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IT/과학']\n"
     ]
    }
   ],
   "source": [
    "# 나이브 베이즈 분류기 사용 예시 3 - IT/과학\n",
    "new_sent = preprocessing([\"20분기 연속으로 적자에 시달리는 LG전자가 브랜드 이름부터 성능, 디자인까지 대대적인 변화를 \\\n",
    "                          적용한 LG 벨벳은 등장 전부터 온라인 커뮤니티를 뜨겁게 달궜다. 사용자들은 “디자인이 예쁘다”, \\\n",
    "                          “슬림하다”는 반응을 보이며 LG 벨벳에 대한 기대감을 드러냈다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.64      0.75       236\n",
      "          사회       0.79      0.91      0.85       422\n",
      "       생활/문화       0.79      0.79      0.79       341\n",
      "\n",
      "    accuracy                           0.81       999\n",
      "   macro avg       0.83      0.78      0.80       999\n",
      "weighted avg       0.82      0.81      0.80       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 나이브 베이즈를 이용한 test data 모델 측정\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 형태소 분석기 변경해 보기 2. Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주 시청 사진 제공 파주시 파주 시청 사진 제공 파주시 파주 강 근주 파주시 일 관내 취약 계층 만 가구 대하 어 정부 긴급 재난 지원금 입금 완료 하 었 다 파주 시민 이 받 긴급 재난 지원금 인 이상 가구 기준 으로 만원 받 게 되 며 이 ㄴ 가구 만원 인 가구 만원 인 가구 만원 이 다 정부 발표 하 ㄴ 긴급 재난 지원금 이 파주시 민 지급 하 ㄴ 금액 과 다르 ㄴ 이유 국비 지방비 부담 비율 때문 이 다 파주시 이미 모든 시민 경기도 파주시 재난 기본 소득 이 ㄴ 당 각 만원 지급 하 고 어 시민 국비 지원금 만 지급 하 며 이 ㄴ 가구 기준 으로 총 지원 금액 파주시 재난 기본 소득 만원 경기도 재난 기본 소득 만원 정부 긴급 재난 지원금 만원 총 만원 받 게 되 ㄴ다 취약 계층 이 아니 ㄴ 시민 오 월일 부터 소지 하 고 신용 체크 카드 사 홈페이지 에서 긴급 재난 지원금 지원 신청 하 ㄹ 수 다 세대주 가족 지원금 일괄 신청 하 어야 하 ㄴ다 한편 파주시 일 김 정기 부시장 단장 으로 하 긴급 재난 지원금 추진 태스크 포스 를 구성 하 어 긴급 재난 지원금 이 원활 하 게 지급 되 ㄹ 수 도록 지원 하 ㄴ다 저작권자 재 배포\n",
      "훈련용 뉴스 기사의 개수 : 2995\n",
      "테스트용 뉴스 기사의 개수 :  999\n",
      "훈련용 레이블의 개수 :  2995\n",
      "테스트용 레이블의 개수 :  999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.75      0.82       236\n",
      "          사회       0.80      0.91      0.85       422\n",
      "       생활/문화       0.81      0.77      0.79       341\n",
      "\n",
      "    accuracy                           0.82       999\n",
      "   macro avg       0.84      0.81      0.82       999\n",
      "weighted avg       0.83      0.82      0.82       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1. 형태소 분석기 변경해 보기 2. Kkma\n",
    "\n",
    "# 2. Kkma 형태소 분석기 사용\n",
    "\n",
    "# 분석기 불러오기\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "# 토큰 분석기 변수에 저장\n",
    "tokenizer_2 = Kkma()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['에','는','은','을','했','에게','있','#','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "        \n",
    "    temp_data = []\n",
    "    \n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer_2.morphs(sentence)\n",
    "    \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "    \n",
    "\n",
    "text_data_2 = preprocessing(news_data_df['news'])\n",
    "print(text_data_2[0])\n",
    "\n",
    "# 사이킷 런 라이브러리를 이용하여 데이터 셋 분리\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data_2, news_data_df['code'], random_state = 0)\n",
    "\n",
    "# 데이터 출력\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "# 나이브 베이즈를 이용한 test data 모델 측정\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1. 형태소 분석기 변경해 보기 3. Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주 시청 사진 제공 파주시 파주 시청 사진 제공 파주시 파주 파이낸셜뉴스 강 근 주 파주시 일 관내 취약 계층 만 가구 대하 아 정부 긴급 재난 지원금 입금 완료 하 았 다 파주시 민이 받 긴급 재난 지원금 인 이상 가구 기준 으로 만원 받 게 되 며 인 가구 만원 인 가구 만원 인 가구 만원 이다 정부 발표 하 ㄴ 긴급 재난 지원금 이 파주시 민 지급 하 ㄴ 금액 과 다른 이유 국비 지방비 부담 비율 때문 이다 파주시 이미 모든 시민 경기도 파주시 재난 기본소득 인당 각 만원 지급 하 고 어 시민 국비 지원금 만 지급 하 며 인 가구 기준 으로 총 지원 금액 파주시 재난 기본소득 만원 경기도 재난 기본소득 만원 정부 긴급 재난 지원금 만원 총 만원 받 게 되 ㄴ 다 취약 계층 이 아니 ㄴ 시민 오 월 일 부터 소지 하 고 신용 체크카드 사 홈페이지 에서 긴급 재난 지원금 지원 신청 하 ㄹ 수 다 세대주 가족 지원금 일괄 신청 하 아야 한 다 한편 파주시 일 김정기 부시장 단장 으로 하 긴급 재난 지원금 추진 태스크포스를 구성 하 아 긴급 재난 지원금 이 원활 하 게 지급 되 ㄹ 수 도록 지원 하 ㄴ다 저작권 자 파이낸셜뉴스 재 배포\n",
      "훈련용 뉴스 기사의 개수 : 2995\n",
      "테스트용 뉴스 기사의 개수 :  999\n",
      "훈련용 레이블의 개수 :  2995\n",
      "테스트용 레이블의 개수 :  999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.74      0.81       236\n",
      "          사회       0.80      0.91      0.85       422\n",
      "       생활/문화       0.81      0.77      0.79       341\n",
      "\n",
      "    accuracy                           0.82       999\n",
      "   macro avg       0.83      0.81      0.82       999\n",
      "weighted avg       0.83      0.82      0.82       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1. 형태소 분석기 변경해 보기 3. Komoran\n",
    "\n",
    "# 3. Komoran 형태소 분석기 사용\n",
    "\n",
    "# 분석기 불러오기\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "# 토큰 분석기 변수에 저장\n",
    "tokenizer_3 = Komoran()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['에','는','은','을','했','에게','있','#','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "        \n",
    "    temp_data = []\n",
    "    \n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer_3.morphs(sentence)\n",
    "    \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "    \n",
    "\n",
    "text_data_3 = preprocessing(news_data_df['news'])\n",
    "print(text_data_3[0])\n",
    "\n",
    "# 사이킷 런 라이브러리를 이용하여 데이터 셋 분리\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data_3, news_data_df['code'], random_state = 0)\n",
    "\n",
    "# 데이터 출력\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "# 나이브 베이즈를 이용한 test data 모델 측정\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 형태소 분석기 변경해 보기 4. Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주 시청 사진 제공 파주시 파주 시청 사진 제공 파주시 파주 파이낸셜뉴스 강 근 주 파주시 일 관내 취약 계층 만 가구 대해 정부 긴급 재난 지 원금 입금 완료 했다 파주시민 이 받는 긴급 재난 지원 금은 인 이상 가구 기준 으로 만원 받게 되며 인 가구 만원 인 가구 만원 인 가구 만원 이다 정부 발표 한 긴급 재난 지 원금 이 파주시민 지급 한 금액 과 다른 이유 국비 지 방비 부담 비율 때문 이다 파주시 이미 모든 시민 경기도 파주시 재난 기본소득 인 당 각 만원 지급 하고 있어 시민 국비 지 원금 만 지급 하며 인 가구 기준 으로 총 지원 금액 파주시 재난 기본소득 만원 경기도 재난 기본소득 만원 정부 긴급 재난 지 원금 만원 총 만원 받게 된다 취약 계층 이 아닌 시민 오는 월일 부터 소지 하고 있는 신용 체크카드 사 홈페이지 에서 긴급 재난 지 원금 지원 신청 할 수 있다 세대주 가족 지 원금 일괄 신청 해야 한다 한편 파주시 일 김정기 부시장 단장 으로 하는 긴급 재난 지 원금 추진 태스크포스 를 구성 해 긴급 재난 지 원금 이 원활하게 지급 될 수 있도록 지원 한 다 저작권 자 파이낸셜뉴스 전 재재 배포\n",
      "훈련용 뉴스 기사의 개수 : 2995\n",
      "테스트용 뉴스 기사의 개수 :  999\n",
      "훈련용 레이블의 개수 :  2995\n",
      "테스트용 레이블의 개수 :  999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.91      0.70      0.79       236\n",
      "          사회       0.78      0.93      0.85       422\n",
      "       생활/문화       0.83      0.76      0.79       341\n",
      "\n",
      "    accuracy                           0.82       999\n",
      "   macro avg       0.84      0.80      0.81       999\n",
      "weighted avg       0.83      0.82      0.82       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1. 형태소 분석기 변경해 보기 4. Okt\n",
    "\n",
    "# 4. Okt 형태소 분석기 사용\n",
    "\n",
    "# 분석기 불러오기\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 토큰 분석기 변수에 저장\n",
    "tokenizer_4 = Okt()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['에','는','은','을','했','에게','있','#','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "        \n",
    "    temp_data = []\n",
    "    \n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer_4.morphs(sentence)\n",
    "    \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "    \n",
    "\n",
    "text_data_4 = preprocessing(news_data_df['news'])\n",
    "print(text_data_4[0])\n",
    "\n",
    "# 사이킷 런 라이브러리를 이용하여 데이터 셋 분리\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data_4, news_data_df['code'], random_state = 0)\n",
    "\n",
    "# 데이터 출력\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "# 나이브 베이즈를 이용한 test data 모델 측정\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 형태소 분석기 변경해 보기 5. Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주 시청 사진제 공파 주시 파주 시청 사진 제공 파주시 파주 강근주 파 주 시 일 관내 취약 계층 만 가구 대해 정부 긴급 재난 지원금 입금 완료 다파 주 시민 이 받 긴급 재난 지원금 인 이상 가구 기준 으로 만 원 받 게 되 며 인 가구 만 원 인 가구 만 원 인 가구 만 원 이 다 정부 발표 한 긴급 재난 지원금 이 파주 시민 지급 한 금액 과 다른 이유 국비 지방비 부담 비율 때문 이 다파 주 시 이미 모든 시민 경기도 파주시 재난 기본 소득 인 당 각 만 원 지급 하 고 어 시민 국비 지원금 만 지급 하 며 인 가구 기준 으로 총 지원 금액 파주시 재난 기본소득 만 원 경기도 재난 기본소득 만 원 정부 긴급 재난 지원금 만 원 총 만 원 받 게 된다 취약 계층 이 아닌 시민 오 월일 부터 소지 하 고 신용 체크카드 사 홈페이지 에서 긴급 재난 지원금 지원 신청 할 수 다 세대주 가족 지원금 일괄 신청 해야 한다 한편 파 주 시 일 김정기 부시장 단장 으로 하 긴급 재난 지원금 추진 태 스 크 포스 를 구성 해 긴급 재난 지원금 이 원활 하 게 지급 될 수 도록 지원 한다 권 자 재 배포\n",
      "훈련용 뉴스 기사의 개수 : 2995\n",
      "테스트용 뉴스 기사의 개수 :  999\n",
      "훈련용 레이블의 개수 :  2995\n",
      "테스트용 레이블의 개수 :  999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.74      0.81       236\n",
      "          사회       0.79      0.91      0.85       422\n",
      "       생활/문화       0.81      0.76      0.79       341\n",
      "\n",
      "    accuracy                           0.82       999\n",
      "   macro avg       0.83      0.80      0.81       999\n",
      "weighted avg       0.82      0.82      0.82       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1. 형태소 분석기 변경해 보기 5. Mecab\n",
    "\n",
    "# 토큰화\n",
    "\n",
    "# 5. Mecab 형태소 분석기 사용\n",
    "\n",
    "# 분석기 불러오기\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# 토큰 분석기 변수에 저장\n",
    "tokenizer_5 = Mecab()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['에','는','은','을','했','에게','있','#','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "        \n",
    "    temp_data = []\n",
    "    \n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer_5.morphs(sentence)\n",
    "    \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "    \n",
    "\n",
    "text_data_5 = preprocessing(news_data_df['news'])\n",
    "print(text_data_5[0])\n",
    "\n",
    "# 사이킷 런 라이브러리를 이용하여 데이터 셋 분리\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data_5, news_data_df['code'], random_state = 0)\n",
    "\n",
    "# 데이터 출력\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "# 나이브 베이즈를 이용한 test data 모델 측정\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. 불용어 추가\n",
    "\n",
    "* 형태소 분석기는 Step 1. 과정에서 상대적으로 F-1 Score Accuracy가 좋게 나온 Kkma와 Komoran 각각 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주 시청 사진 제공 파주시 파주 시청 사진 제공 파주시 파주 강 근주 파주시 일 관내 취약 계층 가구 대하 어 정부 긴급 재난 지원금 입금 완료 었 다 파주 시민 긴급 재난 지원금 이상 가구 기준 만원 되 며 ㄴ 가구 만원 가구 만원 가구 만원 다 정부 발표 ㄴ 긴급 재난 지원금 파주시 민 지급 ㄴ 금액 다르 ㄴ 이유 국비 지방비 부담 비율 때문 다 파주시 모든 시민 경기도 파주시 재난 기본 소득 ㄴ 당 각 만원 지급 어 시민 국비 지원금 지급 며 ㄴ 가구 기준 총 지원 금액 파주시 재난 기본 소득 만원 경기도 재난 기본 소득 만원 정부 긴급 재난 지원금 만원 총 만원 되 ㄴ다 취약 계층 아니 ㄴ 시민 오 월일 부터 소지 신용 체크 카드 사 홈페이지 긴급 재난 지원금 지원 신청 ㄹ 수 다 세대주 가족 지원금 일괄 신청 어야 ㄴ다 한편 파주시 일 김 정기 부시장 단장 긴급 재난 지원금 추진 태스크 포스 구성 어 긴급 재난 지원금 원활 지급 되 ㄹ 수 도록 지원 ㄴ다 저작권자 재 배포\n",
      "훈련용 뉴스 기사의 개수 : 2995\n",
      "테스트용 뉴스 기사의 개수 :  999\n",
      "훈련용 레이블의 개수 :  2995\n",
      "테스트용 레이블의 개수 :  999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.75      0.82       236\n",
      "          사회       0.80      0.91      0.85       422\n",
      "       생활/문화       0.81      0.77      0.79       341\n",
      "\n",
      "    accuracy                           0.82       999\n",
      "   macro avg       0.84      0.81      0.82       999\n",
      "weighted avg       0.83      0.82      0.82       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2. 불용어 추가\n",
    "\n",
    "# 2. Kkma 형태소 분석기 사용\n",
    "\n",
    "# 분석기 불러오기\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "# 토큰 분석기 변수에 저장\n",
    "tokenizer_2 = Kkma()\n",
    "\n",
    "# 불용어 정의 추가 - '으로', '이', '하', '게', '를', '에서', '한', '만', '대해', '받', '인', '과', '하', '고', '이미', '한다', '해야' 추가\n",
    "stopwords = ['에','는','은','을','했','에게','있','#','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스', '으로', '이', '하', '게', '를', '에서', '한', '만', '대해', '받', '인', '과', '하', '고', '이미', '한다', '해야']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "        \n",
    "    temp_data = []\n",
    "    \n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer_2.morphs(sentence)\n",
    "    \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "    \n",
    "\n",
    "text_data_2 = preprocessing(news_data_df['news'])\n",
    "print(text_data_2[0])\n",
    "\n",
    "# 사이킷 런 라이브러리를 이용하여 데이터 셋 분리\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data_2, news_data_df['code'], random_state = 0)\n",
    "\n",
    "# 데이터 출력\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "# 나이브 베이즈를 이용한 test data 모델 측정\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주 시청 사진 제공 파주시 파주 시청 사진 제공 파주시 파주 파이낸셜뉴스 강 근 주 파주시 일 관내 취약 계층 만 가구 대하 아 정부 긴급 재난 지원금 입금 완료 하 았 다 파주시 민이 받 긴급 재난 지원금 인 이상 가구 기준 으로 만원 받 게 되 며 인 가구 만원 인 가구 만원 인 가구 만원 이다 정부 발표 하 ㄴ 긴급 재난 지원금 이 파주시 민 지급 하 ㄴ 금액 과 다른 이유 국비 지방비 부담 비율 때문 이다 파주시 이미 모든 시민 경기도 파주시 재난 기본소득 인당 각 만원 지급 하 고 어 시민 국비 지원금 만 지급 하 며 인 가구 기준 으로 총 지원 금액 파주시 재난 기본소득 만원 경기도 재난 기본소득 만원 정부 긴급 재난 지원금 만원 총 만원 받 게 되 ㄴ 다 취약 계층 이 아니 ㄴ 시민 오 월 일 부터 소지 하 고 신용 체크카드 사 홈페이지 에서 긴급 재난 지원금 지원 신청 하 ㄹ 수 다 세대주 가족 지원금 일괄 신청 하 아야 한 다 한편 파주시 일 김정기 부시장 단장 으로 하 긴급 재난 지원금 추진 태스크포스를 구성 하 아 긴급 재난 지원금 이 원활 하 게 지급 되 ㄹ 수 도록 지원 하 ㄴ다 저작권 자 파이낸셜뉴스 재 배포\n",
      "훈련용 뉴스 기사의 개수 : 2995\n",
      "테스트용 뉴스 기사의 개수 :  999\n",
      "훈련용 레이블의 개수 :  2995\n",
      "테스트용 레이블의 개수 :  999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.74      0.81       236\n",
      "          사회       0.80      0.91      0.85       422\n",
      "       생활/문화       0.81      0.77      0.79       341\n",
      "\n",
      "    accuracy                           0.82       999\n",
      "   macro avg       0.83      0.81      0.82       999\n",
      "weighted avg       0.83      0.82      0.82       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2. 불용어 추가\n",
    "\n",
    "# 3. Komoran 형태소 분석기 사용\n",
    "\n",
    "# 분석기 불러오기\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "# 토큰 분석기 변수에 저장\n",
    "tokenizer_3 = Komoran()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['에','는','은','을','했','에게','있','#','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "        \n",
    "    temp_data = []\n",
    "    \n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer_3.morphs(sentence)\n",
    "    \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "    \n",
    "\n",
    "text_data_3 = preprocessing(news_data_df['news'])\n",
    "print(text_data_3[0])\n",
    "\n",
    "# 사이킷 런 라이브러리를 이용하여 데이터 셋 분리\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data_3, news_data_df['code'], random_state = 0)\n",
    "\n",
    "# 데이터 출력\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "# 나이브 베이즈를 이용한 test data 모델 측정\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. 다른 날짜 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>기사 섹션 분류 안내\\n\\n기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▶제21대 총선 실시간 개표 현황 및 결과 보기\\n\\n총선에서 여당이 다시 한 번 ...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[뉴욕=AP/뉴시스]지난 10일 뉴욕 증권거래소 건물에 미국 국기가 게양되어 있다....</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>부산지역 주유소에서 판매하는 기름값이 휘발유는 평균 1200원대, 경유는 1000원...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>담배업계가 소비자의 취향을 저격한 다양한 담배 신제품들을 잇달아 선보이고 있다.이전...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news code\n",
       "0  기사 섹션 분류 안내\\n\\n기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다...   경제\n",
       "1  ▶제21대 총선 실시간 개표 현황 및 결과 보기\\n\\n총선에서 여당이 다시 한 번 ...   경제\n",
       "2  [뉴욕=AP/뉴시스]지난 10일 뉴욕 증권거래소 건물에 미국 국기가 게양되어 있다....   경제\n",
       "3  부산지역 주유소에서 판매하는 기름값이 휘발유는 평균 1200원대, 경유는 1000원...   경제\n",
       "4  담배업계가 소비자의 취향을 저격한 다양한 담배 신제품들을 잇달아 선보이고 있다.이전...   경제"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 변수에 다른 파일 경로 저장\n",
    "csv_path_2 = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data2.csv\"\n",
    "news_data_df_2 = pd.read_table(csv_path_2, sep=',')\n",
    "news_data_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    기사 섹션 분류 안내기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다 언론사...\n",
       "1    제대 총선 실시간 개표 현황 및 결과 보기총선에서 여당이 다시 한 번 승리를 거두면...\n",
       "2    뉴욕뉴시스지난 일 뉴욕 증권거래소 건물에 미국 국기가 게양되어 있다 신종 코로나바이...\n",
       "3    부산지역 주유소에서 판매하는 기름값이 휘발유는 평균 원대 경유는 원대에 돌입했다일 ...\n",
       "4    담배업계가 소비자의 취향을 저격한 다양한 담배 신제품들을 잇달아 선보이고 있다이전까...\n",
       "Name: news, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거\n",
    "news_data_df_2['news'] = news_data_df_2['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "news_data_df_2['news'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news    0\n",
      "code    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Null 값이 있는지 확인\n",
    "print(news_data_df_2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 기사의 개수:  3703\n",
      "뉴스 기사의 개수:  2137\n"
     ]
    }
   ],
   "source": [
    "# 중복 샘플 제거\n",
    "\n",
    "# 중복 제거 전 기사 갯수\n",
    "print('뉴스 기사의 개수: ',len(news_data_df_2))\n",
    "\n",
    "# 중복 제거\n",
    "news_data_df_2.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "# 중복 제거 후 기사 갯수\n",
    "print('뉴스 기사의 개수: ',len(news_data_df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    code  count\n",
      "0  IT/과학    235\n",
      "1     경제    902\n",
      "2     사회    554\n",
      "3  생활/문화    446\n"
     ]
    }
   ],
   "source": [
    "# 각 카테고리 별 뉴스기사 수 확인\n",
    "print(news_data_df_2.groupby('code').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주 시청 사진 제공 파주시 파주 시청 사진 제공 파주시 파주 파이낸셜뉴스 강 근 주 파주시 일 관내 취약 계층 만 가구 대하 아 정부 긴급 재난 지원금 입금 완료 하 았 다 파주시 민이 받 긴급 재난 지원금 인 이상 가구 기준 으로 만원 받 게 되 며 인 가구 만원 인 가구 만원 인 가구 만원 이다 정부 발표 하 ㄴ 긴급 재난 지원금 이 파주시 민 지급 하 ㄴ 금액 과 다른 이유 국비 지방비 부담 비율 때문 이다 파주시 이미 모든 시민 경기도 파주시 재난 기본소득 인당 각 만원 지급 하 고 어 시민 국비 지원금 만 지급 하 며 인 가구 기준 으로 총 지원 금액 파주시 재난 기본소득 만원 경기도 재난 기본소득 만원 정부 긴급 재난 지원금 만원 총 만원 받 게 되 ㄴ 다 취약 계층 이 아니 ㄴ 시민 오 월 일 부터 소지 하 고 신용 체크카드 사 홈페이지 에서 긴급 재난 지원금 지원 신청 하 ㄹ 수 다 세대주 가족 지원금 일괄 신청 하 아야 한 다 한편 파주시 일 김정기 부시장 단장 으로 하 긴급 재난 지원금 추진 태스크포스를 구성 하 아 긴급 재난 지원금 이 원활 하 게 지급 되 ㄹ 수 도록 지원 하 ㄴ다 저작권 자 파이낸셜뉴스 재 배포\n",
      "훈련용 뉴스 기사의 개수 : 2995\n",
      "테스트용 뉴스 기사의 개수 :  999\n",
      "훈련용 레이블의 개수 :  2995\n",
      "테스트용 레이블의 개수 :  999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.90      0.74      0.81       236\n",
      "          사회       0.80      0.91      0.85       422\n",
      "       생활/문화       0.81      0.77      0.79       341\n",
      "\n",
      "    accuracy                           0.82       999\n",
      "   macro avg       0.83      0.81      0.82       999\n",
      "weighted avg       0.83      0.82      0.82       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step3. 분석 결과 출력\n",
    "# 형태소 분석기 : Komoran 사용\n",
    "# Step 2에서 추가된 불용어 적용\n",
    "\n",
    "# 분석기 불러오기\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "# 토큰 분석기 변수에 저장\n",
    "tokenizer_3 = Komoran()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['에','는','은','을','했','에게','있','#','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']\n",
    "\n",
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "        \n",
    "    temp_data = []\n",
    "    \n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer_3.morphs(sentence)\n",
    "    \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data\n",
    "    \n",
    "\n",
    "text_data_3 = preprocessing(news_data_df['news'])\n",
    "print(text_data_3[0])\n",
    "\n",
    "# 사이킷 런 라이브러리를 이용하여 데이터 셋 분리\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data_3, news_data_df['code'], random_state = 0)\n",
    "\n",
    "# 데이터 출력\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))\n",
    "\n",
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf\n",
    "\n",
    "# 나이브 베이즈를 이용한 test data 모델 측정\n",
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 평가\n",
    "\n",
    "1. 한국어 전처리 과정이 적절하였는가?\n",
    "\n",
    "1의 답변 :\n",
    "형태소 분석 과정은 konlpy에서 지원하는 5개의 형태소 분석기의 결과를 비교하여 가장 성능이 좋은 분석기를 선택함으로써 어느 정도 합리적으로 선택하였다고 생각함. 이후에 각 형태소 분석기의 특징과 원리를 이해한다면, 형태소 분석기 선택에 있어 더 좋은 결정을 할 수 있다고 생각됨\n",
    "\n",
    "불용어 제거 과정의 경우, 불용어 선택에 있어 제 주관적인 선택에 의존하였으므로, 불용어 선택과정은 최적화 되지 않았다고 생각함\n",
    "\n",
    "2. 텍스트 데이터 수집이 분량과 다양성 측면에서 적절했는가?\n",
    "일자와 분량에서 텍스트 데이터 다양성 향상을 위한 노력이 확인됨\n",
    "\n",
    "3. 분류모델의 test accuracy가 기준 이상 높게 나왔는가?\n",
    "F-1 score 기준 82% 이상의 정확도가 확인됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
